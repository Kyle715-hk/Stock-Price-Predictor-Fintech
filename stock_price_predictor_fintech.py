# -*- coding: utf-8 -*-
"""Stock-Price-Predictor-Fintech

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VI6droqqBiOeI3m9HUTyaOJZaT_wdB1Q
"""

!pip install alpha_vantage

pip install streamlit

# --------------------------------------------------------------
#  Step 2 – Hybrid Data Pipeline (Alpha Vantage + yfinance)
#  → TSLA (Alpha Vantage) + VIX & 10-yr Treasury (yfinance)
# --------------------------------------------------------------

import time
import requests
import pandas as pd
import yfinance as yf
from alpha_vantage.timeseries import TimeSeries

# ---------- 1. INSERT YOUR API KEY (hard-coded) ----------
API_KEY = "PASTE_YOUR_ALPHA_VANTAGE_KEY_HERE"

# ---------- 2. Parameters ----------
STOCK   = "TSLA"
START   = "2015-11-14"
END     = "2025-11-15"          # today
ts = TimeSeries(key=API_KEY, output_format="pandas")

# --------------------------------------------------------------
# 1. TSLA – Alpha Vantage (historical + real-time)
print("Fetching TSLA historical + real-time (Alpha Vantage)...")
stock_df, _ = ts.get_daily(symbol=STOCK, outputsize="full")
stock_df = stock_df.sort_index()
stock_df.to_csv(f"{STOCK}_historical_data.csv")
print(f"TSLA historical rows: {len(stock_df)}")

# ---- real-time quote ----
quote_url = f"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={STOCK}&apikey={API_KEY}"
quote_json = requests.get(quote_url).json()
quote = quote_json.get("Global Quote", {})

if quote:
    latest_date = pd.to_datetime(quote["07. latest trading day"])
    latest_row = pd.DataFrame({
        "1. open":   [float(quote["02. open"])],
        "2. high":   [float(quote["03. high"])],
        "3. low":    [float(quote["04. low"])],
        "4. close":  [float(quote["05. price"])],
        "5. volume": [int(quote["06. volume"])]
    }, index=[latest_date])

    if latest_date in stock_df.index:
        stock_df.loc[latest_date] = latest_row.loc[latest_date]
    else:
        stock_df = pd.concat([stock_df, latest_row])
    print(f"Realtime TSLA updated: {latest_date.date()} Close: {quote['05. price']}")
else:
    print("Warning: No real-time quote – using last historical close.")
stock_df.to_csv(f"{STOCK}_historical_with_realtime.csv")

# --------------------------------------------------------------
# 2. VIX – yfinance (explicit auto_adjust=False)
print("\nFetching VIX (yfinance)...")
vix_raw = yf.download("^VIX", start=START, end=END, progress=False, auto_adjust=False)
if vix_raw.empty:
    raise ValueError("No VIX data returned.")
vix_df = vix_raw[['Close']].rename(columns={'Close': 'VIX'})   # <-- simple column name
vix_df.to_csv("vix_historical_data.csv")
print(f"VIX rows: {len(vix_df)}")

# --------------------------------------------------------------
# 3. 10-Year Treasury Yield – yfinance
print("\nFetching 10-Year Treasury Yield (yfinance)...")
rates_raw = yf.download("^TNX", start=START, end=END, progress=False, auto_adjust=False)
if rates_raw.empty:
    raise ValueError("No Treasury data returned.")
rates_df = rates_raw[['Close']].rename(columns={'Close': 'Interest_Rate'})
rates_df.to_csv("interest_rates_historical_data.csv")
print(f"Treasury rows: {len(rates_df)}")

# --------------------------------------------------------------
# 4. Combine & preview (last 5 rows of the 10-year window)
combined = pd.concat([
    stock_df["4. close"].rename("Stock_Close"),
    vix_df["VIX"],
    rates_df["Interest_Rate"]
], axis=1).dropna()

combined = combined[combined.index >= START]
print("\n=== Last 5 Rows (10-Year Window) ===")
print(combined.tail())

# --------------------------------------------------------------
# Step 3: Preprocess the Data
# - Merge datasets into a single dataframe
# - Handle missing values with forward fill and drop any remaining NaNs
# - Engineer features: e.g., 50-day moving average on stock close
# - Normalize all features using MinMaxScaler for LSTM compatibility
# - Create time series sequences (60-day windows) for multi-feature input
# - Split into train/test sets (80/20)
# --------------------------------------------------------------

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Assuming 'combined' from Step 2 is available; if not, load from CSVs
# combined = pd.read_csv('combined_data.csv', index_col=0, parse_dates=True)  # Uncomment if needed

# Copy and prepare dataframe
df = combined.copy()

# Handle any missing values (e.g., holidays where data might differ)
df = df.ffill().dropna()  # Forward fill then drop remaining NaNs

# Feature engineering: Add 50-day moving average on Stock_Close
df['MA_50'] = df['Stock_Close'].rolling(window=50).mean()

# Drop rows with NaN from rolling (first 49 rows)
df = df.dropna()

# Define feature columns (target is Stock_Close, others are predictors)
features = ['Stock_Close', 'MA_50', '^VIX', '^TNX']

# Scale features to [0, 1] for LSTM
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df[features])

# Save scaler for later inverse transformations (e.g., predictions)
import joblib
joblib.dump(scaler, 'scaler.pkl')

# Function to create sequences: 60-day lookback, predict next Stock_Close
def create_sequences(data, seq_length=60):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        xs.append(data[i:i + seq_length])  # Shape: (seq_length, n_features)
        ys.append(data[i + seq_length, 0])  # Target: next scaled Stock_Close (index 0)
    return np.array(xs), np.array(ys)

# Generate X (inputs) and y (targets)
X, y = create_sequences(scaled_data)

# Split: 80% train, 20% test
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Preview shapes
print(f"X_train shape: {X_train.shape}  # (samples, timesteps, features)")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# Optional: Save preprocessed data for later steps
np.save('X_train.npy', X_train)
np.save('y_train.npy', y_train)
np.save('X_test.npy', X_test)
np.save('y_test.npy', y_test)

# --------------------------------------------------------------
# Step 4: Build and Train the LSTM Model
# - Use TensorFlow/Keras to create a sequential LSTM model
# - Architecture: 2 LSTM layers with dropout for regularization
# - Compile: Adam optimizer, Mean Squared Error loss
# - Train: 50 epochs, batch size 32, 20% validation split
# - Save the trained model for evaluation and deployment
# - Assumes X_train, y_train from Step 3 are available
# --------------------------------------------------------------

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Build the model
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=1))  # Output: predicted next scaled Stock_Close

# Compile
model.compile(optimizer='adam', loss='mean_squared_error')

# Print model summary
model.summary()

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=1  # Show progress
)

# Save the trained model
model.save('stock_predictor_lstm_model.h5')

# Optional: Plot training history (loss curves)
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('LSTM Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error Loss')
plt.legend()
plt.savefig('training_history.png')  # Save for README/Github
plt.show()

# --------------------------------------------------------------
# Step 5: Evaluate the Model
# - Load the trained model if needed (assumes it's already in memory from Step 4)
# - Make predictions on test set
# - Inverse scale predictions and actuals back to original values
# - Compute evaluation metrics: RMSE (primary), MAE as secondary
# - Visualize: Plot actual vs. predicted stock prices
# - Save plot for GitHub/README
# - Assumes model, X_test, y_test, scaler from previous steps are available
# --------------------------------------------------------------

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np
import joblib

# Load model if not in memory (e.g., from saved file)
# model = tf.keras.models.load_model('stock_predictor_lstm_model.h5')  # Uncomment if needed

# Load scaler (from Step 3)
scaler = joblib.load('scaler.pkl')

# Make predictions on test set (scaled)
predictions_scaled = model.predict(X_test)

# Prepare for inverse scaling: Create dummy array with same shape as scaled_data
# (Only need to inverse the Stock_Close column, index 0)
dummy = np.zeros((len(predictions_scaled), scaler.n_features_in_))
dummy[:, 0] = predictions_scaled.flatten()  # Insert predictions into first column
predictions = scaler.inverse_transform(dummy)[:, 0]  # Extract inversed Stock_Close

# Inverse scale actual y_test (similarly)
dummy[:, 0] = y_test
actual = scaler.inverse_transform(dummy)[:, 0]

# Compute metrics
rmse = np.sqrt(mean_squared_error(actual, predictions))
mae = mean_absolute_error(actual, predictions)
print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
print(f'Mean Absolute Error (MAE): {mae:.2f}')

# Visualize: Actual vs Predicted
plt.figure(figsize=(14, 6))
plt.plot(df.index[-len(actual):], actual, label='Actual Stock Price', color='blue')
plt.plot(df.index[-len(predictions):], predictions, label='Predicted Stock Price', color='red', linestyle='--')
plt.title('Stock Price Prediction: Actual vs Predicted')
plt.xlabel('Date')
plt.ylabel('Stock Price (USD)')
plt.legend()
plt.grid(True)
plt.savefig('prediction_vs_actual.png')  # Save for portfolio
plt.show()

# --------------------------------------------------------------
# Step 6: Perform Sensitivity Analysis
# - Test model robustness under varying volatility and interest rate scenarios
# - Volatility: Add Gaussian noise to test inputs at different levels
# - Interest Rates: Simulate rate changes with linear adjustments to stock prices
# - Compute RMSE for each scenario on test set
# - Document findings for README (e.g., impact on prediction accuracy)
# - Assumes model, scaler, df, X_test, y_test from previous steps are available
# --------------------------------------------------------------

import numpy as np
from sklearn.metrics import mean_squared_error
import joblib

# Load scaler if needed (from Step 3)
scaler = joblib.load('scaler.pkl')

# Function to generate perturbed test data
def perturb_data(data, feature_idx, perturbation_func, **kwargs):
    perturbed = data.copy()
    perturbed[:, :, feature_idx] = perturbation_func(perturbed[:, :, feature_idx], **kwargs)
    return perturbed

# Volatility sensitivity: Add Gaussian noise to VIX feature (index 2 in features: ['Stock_Close', 'MA_50', 'VIX', 'Interest_Rate'])
def add_volatility(feature_data, level):
    noise = np.random.normal(0, level, feature_data.shape)
    return feature_data + noise * feature_data  # Relative noise

print("\n=== Volatility Sensitivity Analysis ===")
vol_levels = [0.05, 0.10, 0.15]  # 5%, 10%, 15% noise levels
for level in vol_levels:
    X_vol = perturb_data(X_test, feature_idx=2, perturbation_func=add_volatility, level=level)

    # Predict on perturbed data
    pred_vol_scaled = model.predict(X_vol)

    # Inverse scale predictions and actuals
    dummy = np.zeros((len(pred_vol_scaled), scaler.n_features_in_))
    dummy[:, 0] = pred_vol_scaled.flatten()
    pred_vol = scaler.inverse_transform(dummy)[:, 0]

    dummy[:, 0] = y_test
    actual = scaler.inverse_transform(dummy)[:, 0]

    rmse_vol = np.sqrt(mean_squared_error(actual, pred_vol))
    print(f'RMSE at {level*100:.0f}% volatility noise: {rmse_vol:.2f}')

# Interest rate sensitivity: Linear adjustment to Stock_Close and MA_50 (indices 0 and 1)
def adjust_rates(feature_data, rate_change):
    return feature_data * (1 - rate_change)  # Assume inverse impact (higher rates lower prices)

print("\n=== Interest Rate Sensitivity Analysis ===")
rate_changes = [0.01, 0.03, 0.05]  # 1%, 3%, 5% rate hikes
for change in rate_changes:
    # Perturb Stock_Close (0) and MA_50 (1); rates themselves (3) remain unchanged
    X_rate = perturb_data(X_test, feature_idx=0, perturbation_func=adjust_rates, rate_change=change)
    X_rate = perturb_data(X_rate, feature_idx=1, perturbation_func=adjust_rates, rate_change=change)

    # Predict
    pred_rate_scaled = model.predict(X_rate)

    # Inverse scale (note: actuals remain unperturbed for fair comparison)
    dummy[:, 0] = pred_rate_scaled.flatten()
    pred_rate = scaler.inverse_transform(dummy)[:, 0]

    rmse_rate = np.sqrt(mean_squared_error(actual, pred_rate))
    print(f'RMSE at {change*100:.0f}% rate change: {rmse_rate:.2f}')

# Optional: Save results to text file for README inclusion
with open('sensitivity_results.txt', 'w') as f:
    f.write("Volatility Sensitivity:\n")
    for level in vol_levels:
        f.write(f'{level*100:.0f}%: RMSE = {rmse_vol:.2f}\n')
    f.write("\nInterest Rate Sensitivity:\n")
    for change in rate_changes:
        f.write(f'{change*100:.0f}%: RMSE = {rmse_rate:.2f}\n')
print("\nSensitivity results saved to 'sensitivity_results.txt'")

import subprocess

# Step 7 code as a string (copied from previous response)
step7_code = '''
import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
from tensorflow.keras.models import load_model
import joblib
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

# Load trained model and scaler
@st.cache_resource
def load_assets():
    model = load_model('stock_predictor_lstm_model.h5')
    scaler = joblib.load('scaler.pkl')
    return model, scaler

model, scaler = load_assets()

# Functions from previous steps (adapted for app)
def fetch_data(ticker, start='2015-11-14', end='2025-11-15'):
    """Fetch stock, VIX, rates data using yfinance (fallback for simplicity in app)"""
    stock_raw = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=False)
    vix_raw = yf.download('^VIX', start=start, end=end, progress=False, auto_adjust=False)
    rates_raw = yf.download('^TNX', start=start, end=end, progress=False, auto_adjust=False)

    combined = pd.concat([
        stock_raw['Close'].rename('Stock_Close'),
        vix_raw['Close'].rename('VIX'),
        rates_raw['Close'].rename('Interest_Rate')
    ], axis=1).ffill().dropna()  # Simple merge with forward fill

    return combined

def preprocess_data(df, seq_length=60):
    df['MA_50'] = df['Stock_Close'].rolling(window=50).mean()
    df = df.dropna()

    features = ['Stock_Close', 'MA_50', 'VIX', 'Interest_Rate']
    scaled_data = scaler.transform(df[features])

    X = []
    for i in range(len(scaled_data) - seq_length):
        X.append(scaled_data[i:i + seq_length])
    return np.array(X)

# Streamlit App
st.title('Fintech Stock Predictor: Simulate Revolut Investing Tools')
st.markdown("""
This app forecasts stock prices using an LSTM model, with sensitivity to volatility and rates.
Inspired by Revolut's user-facing investment features for real-time decisions.
""")

# User inputs
ticker = st.text_input('Enter Stock Ticker (e.g., TSLA or AAPL)', value='TSLA')
if st.button('Run Prediction'):

    with st.spinner('Fetching data and predicting...'):
        try:
            # Fetch and preprocess
            df = fetch_data(ticker)
            if df.empty:
                raise ValueError("No data fetched. Check ticker.")

            X = preprocess_data(df)
            if len(X) == 0:
                raise ValueError("Insufficient data for sequences.")

            # Predict (use last sequence for next-day forecast)
            last_seq = X[-1:]  # Shape: (1, 60, 4)
            pred_scaled = model.predict(last_seq)

            # Inverse scale
            dummy = np.zeros((1, scaler.n_features_in_))
            dummy[0, 0] = pred_scaled[0]
            pred_price = scaler.inverse_transform(dummy)[0, 0]

            last_price = df['Stock_Close'].iloc[-1]
            st.success(f'Predicted Next-Day Price for {ticker}: ${pred_price:.2f} (Current: ${last_price:.2f})')

            # Quick evaluation on historical test (simplified)
            predictions = model.predict(X)
            dummy = np.zeros((len(predictions), scaler.n_features_in_))
            dummy[:, 0] = predictions.flatten()
            preds = scaler.inverse_transform(dummy)[:, 0]

            actual = df['Stock_Close'].iloc[60:].values  # Align with sequences
            rmse = np.sqrt(mean_squared_error(actual, preds))
            st.info(f'Historical RMSE: {rmse:.2f}')

            # Plot
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.plot(df.index[60:], actual, label='Actual')
            ax.plot(df.index[60:], preds, label='Predicted', linestyle='--')
            ax.set_title(f'{ticker} Price Prediction')
            ax.legend()
            st.pyplot(fig)

            # Sensitivity (simplified for app: show one scenario)
            st.subheader('Sensitivity Example (10% Volatility Noise)')
            vol_level = 0.10
            X_vol = X.copy()
            X_vol[:, :, 2] += np.random.normal(0, vol_level, X_vol[:, :, 2].shape) * X_vol[:, :, 2]  # Perturb VIX

            preds_vol = model.predict(X_vol)
            dummy[:, 0] = preds_vol.flatten()
            preds_vol_inv = scaler.inverse_transform(dummy)[:, 0]

            rmse_vol = np.sqrt(mean_squared_error(actual, preds_vol_inv))
            st.write(f'RMSE under 10% volatility: {rmse_vol:.2f}')

        except Exception as e:
            st.error(f'Error: {str(e)}. Try another ticker or check data availability.')
'''

# Save to app.py
with open('app.py', 'w') as f:
    f.write(step7_code.strip())

print("Saved Step 7 code to 'app.py'.")

# Launch the app (opens in browser)
subprocess.run(['streamlit', 'run', 'app.py'])